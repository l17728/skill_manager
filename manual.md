# SkillManager 用户手册

**版本 1.0 · 适用平台：Windows 10 / Windows 11**

---

## 目录

1. [产品简介](#1-产品简介)
2. [系统要求](#2-系统要求)
3. [安装与启动](#3-安装与启动)
4. [界面总览](#4-界面总览)
5. [Skills & Agents 管理](#5-skills--agents-管理)
6. [测试基线（Baselines）管理](#6-测试基线baselines管理)
7. [测试项目（Projects）管理](#7-测试项目projects管理)
8. [自动化对比测试](#8-自动化对比测试)
9. [差异分析与优势识别](#9-差异分析与优势识别)
10. [Skill 优势重组](#10-skill-优势重组)
11. [迭代验证闭环](#11-迭代验证闭环)
12. [Rankings 成绩排行榜](#12-rankings-成绩排行榜)
13. [数据管理与备份](#13-数据管理与备份)
14. [常见问题排查](#14-常见问题排查)

---

## 1. 产品简介

**SkillManager** 是一款面向 AI 提示词工程师和 Agent 开发者的桌面工作台。它帮助你：

- **统一管理** 来自不同来源的 Skill（提示词）和 Agent
- **客观测试** 多个 Skill 在同一批用例下的表现，得到可量化的评分
- **自动分析** 哪个 Skill 更好、好在哪里
- **智能重组** 融合多个 Skill 的优势，生成更强的新版本
- **持续迭代** 自动循环"重组 → 测试 → 分析"，直到达到满意分数
- **成绩排行** 一览所有 Skill 的历史测试成绩和新鲜度状态

> **核心理念**：所有数据以文件形式存储在本地，无需数据库；所有 AI 调用通过 Claude Code CLI 完成，结果完全可复现、可追溯。

---

## 2. 系统要求

| 项目 | 要求 |
|------|------|
| 操作系统 | Windows 10 / Windows 11（64 位） |
| Node.js | ≥ 18.0.0（推荐最新 LTS） |
| npm | ≥ 8.0.0（随 Node.js 附带） |
| Claude Code CLI | 最新版，`claude` 命令须在系统 PATH 中可用 |
| 磁盘空间 | ≥ 500 MB（含依赖库） |
| 网络 | 执行测试/自动打标签时需能访问 Anthropic API |

---

## 3. 安装与启动

### 3.1 第一步：安装 Node.js

1. 访问 [https://nodejs.org](https://nodejs.org)，下载 **LTS** 版本（`.msi` 文件）
2. 运行安装包，保持默认选项，确保勾选 **"Add to PATH"**
3. 安装完成后打开新的命令提示符（CMD），验证：

```
node --version
npm --version
```

两条命令均有版本号输出即为安装成功。

### 3.2 第二步：安装 Claude Code CLI

SkillManager 的测试、分析、自动打标签等核心功能均通过 Claude Code CLI 执行。

在命令提示符中运行：

```
npm install -g @anthropic-ai/claude-code
```

安装后验证：

```
claude --version
```

**授权登录**（首次使用必须完成）：

```
claude auth login
```

按提示完成 Anthropic 账户授权。未授权时，资产管理功能（导入/编辑 Skill 和 Baseline）可正常使用，但测试、分析、自动打标签等功能无法执行。

### 3.3 第三步：获取 SkillManager 并安装依赖

```
git clone https://github.com/l17728/skill_manager.git
cd skill_manager
npm install
```

`npm install` 会自动下载所有依赖，完成后约占 300–400 MB。

### 3.4 第四步：启动应用

**开发模式**（含调试工具，推荐首次使用）：

```
npm run dev
```

**生产模式**：

```
npm start
```

应用窗口弹出即启动成功。首次启动时，程序会自动在项目目录下创建 `workspace/` 文件夹，用于存放所有数据。

### 3.5 打包为独立安装包（可选）

如需将应用分发给他人，可打包为 Windows 安装程序：

```
npm run build
```

输出文件位于 `dist/` 目录，双击 `.exe` 文件即可在其他 Windows 电脑上安装使用（目标电脑仍需安装 Claude Code CLI）。

---

## 4. 界面总览

启动后，界面分为以下几个区域：

```
┌─────────────────────────────────────────────────────────┐
│  ⚡ SkillManager  [Skills & Agents] [Baselines]          │
│                   [Projects] [Rankings]      CLI: v1.x.x │
├─────────────────────────────────────────────────────────┤
│                                                         │
│               主内容区（随导航切换）                      │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### 顶部导航栏

| 元素 | 说明 |
|------|------|
| **Skills & Agents** | 管理所有提示词和 Agent 资产 |
| **Baselines** | 管理测试基线（用例集合） |
| **Projects** | 创建和管理测试项目 |
| **Rankings** | 查看所有 Skill 的历史成绩排行 |
| **CLI 状态指示灯** | 绿色=CLI 正常，红色=CLI 不可用 |

### 页面布局模式

大多数页面采用**左右分栏**布局：
- **左栏**：列表/筛选面板
- **右栏**：详情/操作面板

---

## 5. Skills & Agents 管理

> **什么是 Skill？** Skill 是一段提示词（Prompt），用于指导 Claude 完成特定任务。Agent 是功能更复杂的提示词，两者在本软件中用同一套管理逻辑。

### 5.1 导入 Skill

1. 点击顶部导航 **Skills & Agents**
2. 点击右侧详情区或列表区的 **+ Import** 按钮
3. 在弹窗中填写信息：

| 字段 | 说明 | 是否必填 |
|------|------|---------|
| **Name（名称）** | Skill 的显示名称 | ✅ 必填 |
| **Content（内容）** | 粘贴提示词全文 | ✅ 必填 |
| **Purpose（用途）** | 如 `coding`、`writing`、`analysis` | ✅ 必填 |
| **Provider（提供方）** | 如 `anthropic`、`openai`、`internal` | ✅ 必填 |
| **Type（类型）** | `skill`（提示词）或 `agent`（智能体） | 可选，默认 skill |
| **Description（描述）** | 简短说明 Skill 的功能 | 可选 |
| **Author（作者）** | 创建者姓名 | 可选 |

4. 填写完成后点击 **Confirm Import**，成功后列表自动刷新

> **提示**：Purpose 和 Provider 决定了 Skill 在 `workspace/` 目录下的存储路径，相同 Purpose 下的不同 Provider 会分层归档，便于管理。

### 5.2 浏览与搜索

**列表区域**位于左侧，支持：

- **搜索**：在顶部搜索框输入关键词，按名称或描述实时过滤
- **标签筛选**：在 Tag 过滤框输入标签，按回车添加筛选条件，可叠加多个
- **Purpose / Provider 筛选**：在对应输入框中输入过滤条件
- **分页**：列表底部显示页码，点击 `< >` 翻页

**Hover 预览**：鼠标悬停在列表项上，会弹出包含提示词核心内容的预览框。

**成绩 Badge**：有测试记录的 Skill 在列表项右侧会显示彩色分数徽标（如 `✓ 85`）：
- 🟢 绿色：当前版本有效成绩
- 🟡 琥珀色：Skill 已更新但旧版本有成绩
- ⚫ 灰色：基线已更新，成绩仅供参考

点击 Badge 可直接跳转到 Rankings 页并以该 Skill 过滤。

### 5.3 查看详情

点击列表中的某个 Skill，右侧详情面板显示：
- 完整提示词内容
- 元数据（名称、用途、提供方、类型、版本等）
- 标签列表（手动标签 + 已审核的自动标签）
- 版本历史记录

### 5.4 编辑 Skill

1. 在列表中选中某个 Skill
2. 点击详情面板的 **Edit** 按钮
3. 修改提示词内容或元数据
4. 点击 **Save**

> **版本管理**：每次保存都会自动生成新版本（v1 → v2 → v3…）。系统会记录修改前后的差异（diff），可在版本历史中查看。

### 5.5 版本历史与回滚

在详情面板的 **History** 区域可以：
- 查看所有历史版本列表
- 查看任意两个版本之间的变更内容（diff）
- 点击 **Rollback** 按钮回滚到指定版本（回滚会创建一个新版本，不会覆盖历史）

### 5.6 标签管理

**手动标签**：
1. 在详情面板的标签区域，点击 **+ Add Tag**
2. 输入标签内容，按回车确认
3. 点击标签旁的 × 可删除

**自动打标签**（需要 Claude CLI 在线）：
1. 在列表中选中 Skill
2. 点击 **Auto Tag** 按钮
3. 确认后系统调用 Claude CLI 分析内容，生成候选标签
4. 生成完成后，标签状态显示为**待审核**
5. 在详情面板的自动标签区域，对每个候选标签点击 ✓ 确认 或 × 拒绝
6. 确认的标签自动生效并参与搜索过滤

### 5.7 删除 Skill

1. 选中列表中的 Skill
2. 点击 **Archive**（归档）按钮
3. 在弹出的确认框中确认操作

> **注意**：归档后 Skill 从列表中隐藏，但文件仍保留在磁盘上，不会物理删除。

---

## 6. 测试基线（Baselines）管理

> **什么是测试基线？** 基线是一组标准测试用例，每个用例包含"输入"和"预期输出"。用基线测试 Skill，就像用试卷测试学生——不同 Skill 面对同样的题目，谁答得更好一目了然。

### 6.1 导入基线

1. 点击顶部导航 **Baselines**
2. 点击 **+ Import** 按钮
3. 选择导入方式：

**手动录入**：
- 填写基线名称、用途、提供方
- 在用例区域逐条添加测试用例
- 每条用例包含：用例名称、输入内容、预期输出

**文件导入**：
- 上传 `.json` 格式的用例文件

4. 填写完成后点击 **Confirm Import**

### 6.2 浏览与管理基线

与 Skills 页面布局一致：左侧列表，右侧详情。

详情面板包含：
- 基线基本信息（名称、用途、提供方、版本、用例数量）
- 所有测试用例列表
- 标签管理区域
- 版本历史

### 6.3 管理测试用例

在基线详情面板中：
- 点击 **+ Add Case** 新增用例
- 点击用例右侧的编辑图标修改
- 点击删除图标移除该用例

> **提示**：修改用例后会自动生成新版本，旧版本用例记录不会丢失。

### 6.4 基线版本管理

基线的版本管理逻辑与 Skill 完全一致，支持查看 diff 和回滚。

---

## 7. 测试项目（Projects）管理

> **什么是测试项目？** 项目是一次完整的对比测试实验，将多个 Skill 和一组测试基线绑定在一起，统一执行并记录结果。

### 7.1 创建项目

1. 点击顶部导航 **Projects**
2. 点击 **+ Create Project**
3. 在弹窗中填写：

| 字段 | 说明 |
|------|------|
| **Project Name** | 项目名称（如 "Python 代码生成对比 2024-06"） |
| **Skills** | 从列表中勾选要参与测试的 Skill（至少 1 个） |
| **Baselines** | 从列表中勾选测试基线（至少 1 个） |

4. 点击 **Create**，系统自动：
   - 生成独立的项目目录
   - 复制所选 Skill 和 Baseline 的副本（测试期间原资产修改不影响项目）
   - 创建独立的 Claude 会话目录（`.claude/`）

### 7.2 项目状态说明

| 状态 | 含义 |
|------|------|
| **未开始** | 项目已创建，测试尚未启动 |
| **运行中** | 测试正在执行 |
| **已完成** | 所有用例测试完毕，结果已归档 |
| **中断** | 测试中途被暂停或异常中断 |

### 7.3 项目详情页

点击项目列表中的某个项目，右侧详情面板有 5 个标签页：

| 标签 | 功能 |
|------|------|
| **Overview** | 查看项目配置（Skills 列表、Baseline 列表、创建时间等） |
| **Test** | 执行和监控测试，查看测试结果 |
| **Analysis** | 查看差异分析报告 |
| **Recompose** | 执行 Skill 优势重组 |
| **Iteration** | 配置并运行迭代验证闭环 |

### 7.4 搜索与删除项目

- **搜索**：左侧列表顶部搜索框，按项目名称过滤
- **删除**：选中项目后点击 **Delete** 按钮，确认后删除（此操作不可恢复）

---

## 8. 自动化对比测试

> 在项目详情的 **Test** 标签页中执行。

### 8.1 开始测试

1. 进入项目详情页，点击 **Test** 标签
2. 点击 **▶ Start Test** 按钮
3. 系统开始依次对每个 Skill 执行基线中的所有用例

测试过程中可以看到：
- **进度条**：已完成用例数 / 总用例数
- **实时日志**：每条用例的执行状态

### 8.2 暂停与恢复

- 点击 **⏸ Pause** 暂停测试（已完成的用例结果保留）
- 点击 **▶ Resume** 从暂停处继续执行（不会重复执行已完成的用例）
- 点击 **⏹ Stop** 终止测试

### 8.3 查看测试结果

测试完成后，Test 页面显示结果汇总：

**排名表格**（Ranking）：
- 每行代表一个 Skill
- 包含：排名、Skill 名称、平均得分、完成用例数、失败用例数
- 点击某行展开 **6 维度得分明细**：

| 维度 | 满分 | 评判要点 |
|------|------|---------|
| 功能正确性 | 30 分 | 代码是否准确实现需求 |
| 健壮性 | 20 分 | 异常处理是否完善 |
| 代码可读性 | 15 分 | 命名规范、结构清晰 |
| 代码简洁性 | 15 分 | 无冗余、表达高效 |
| 复杂度控制 | 10 分 | 避免不必要的复杂度 |
| 格式规范性 | 10 分 | 符合语言编码规范 |

> **得分颜色含义**：🟢 绿色（≥80% 满分）· 🟡 黄色（≥60%）· 🔴 红色（<60%）

### 8.4 用例失败处理

如果某条用例执行失败，该用例标记为 **Failed**，但不影响其他用例继续执行。失败原因包括：CLI 超时、网络错误等，可查看详细日志排查。

---

## 9. 差异分析与优势识别

> 在项目详情的 **Analysis** 标签页中执行。测试完成后方可进行分析。

### 9.1 运行分析

1. 确保项目已完成测试
2. 进入 **Analysis** 标签页
3. 点击 **▶ Run Analysis**
4. 系统调用 Claude CLI 自动分析各 Skill 的差异，生成报告

### 9.2 分析报告内容

分析完成后，报告包含：

- **最优 Skill 推荐**：综合得分最高的 Skill 及推荐理由
- **各维度领先者**：每个评分维度上表现最好的 Skill
- **优势片段提取**：从每个 Skill 中提取的关键优势片段，按类型分类：
  - 指令结构
  - 约束条件
  - 输出格式
  - 角色设定
  - 示例内容

这些优势片段将作为后续重组的素材。

### 9.3 导出分析报告

点击 **Export Report** 可将分析报告导出为 JSON 文件，保存到 `workspace/` 目录。

---

## 10. Skill 优势重组

> 在项目详情的 **Recompose** 标签页中执行。分析完成后方可进行重组。

重组功能将多个 Skill 中的最优片段融合，自动生成一个更强的新 Skill。

### 10.1 执行重组

1. 进入 **Recompose** 标签页
2. 点击 **▶ Execute Recompose**
3. 系统调用 Claude CLI，根据分析报告中的优势片段自动重组

重组完成后，预览区显示生成的新 Skill 完整内容。

### 10.2 编辑重组结果

重组完成后，你可以：
- 直接在预览框中**手动编辑**内容
- 对不满意的部分进行修改

### 10.3 保存重组结果

填写新 Skill 的名称、用途等信息，点击 **Save as New Skill**：
- 新 Skill 自动保存到 Skills & Agents 列表
- 包含完整溯源信息（记录融合自哪些 Skill）

---

## 11. 迭代验证闭环

> 在项目详情的 **Iteration** 标签页中执行。

迭代功能实现"重组 → 测试 → 分析 → 再重组"的自动闭环，持续优化 Skill 直到达到目标分数。

### 11.1 配置迭代参数

在 **Iteration** 标签页中设置：

| 参数 | 说明 | 默认值 |
|------|------|--------|
| **Max Rounds（最大轮次）** | 最多迭代几轮 | 3 |
| **Stop Threshold（停止阈值）** | 平均分达到此值自动停止 | 90 |
| **Mode（模式）** | 选择迭代模式 | Standard |

**三种迭代模式**：

| 模式 | 说明 | 适用场景 |
|------|------|---------|
| **Standard** | 每轮生成 1 个候选，标准推进 | 快速迭代，资源消耗少 |
| **Explore** | 每轮生成 2 个候选，选最优 | 需要探索更多可能性 |
| **Adaptive** | 智能检测停滞，激进突破 | 分数卡在瓶颈，需要突破 |

### 11.2 启动迭代

点击 **▶ Start Iteration**，系统自动执行：

```
第 1 轮：重组 → 测试 → 分析
第 2 轮：基于第 1 轮结果再次重组 → 测试 → 分析
……
直到达到最大轮次或停止阈值
```

迭代过程中可以看到每轮的得分变化。

### 11.3 查看迭代报告

迭代完成后，报告包含：
- 每轮的 Skill 内容和得分
- 分数变化趋势
- 最优版本推荐
- 各轮策略说明

点击 **Save Best Skill** 将最优版本保存为新 Skill。

### 11.4 暂停与停止

- 点击 **⏸ Pause** 在当前轮结束后暂停
- 点击 **⏹ Stop** 立即终止迭代（已完成轮次的结果保留）

---

## 12. Rankings 成绩排行榜

> 点击顶部导航 **Rankings** 进入。

Rankings 页面汇总所有已完成测试的成绩，帮助你纵观所有 Skill 的历史表现。

### 12.1 页面布局

```
┌──────────────┬────────────────────────────────────┐
│   Rankings   │   排名列表 / 时间线图表             │
│              │                                    │
│  🔍 搜索...  │  ┌─ Python Coding Eval ──────────┐ │
│  [全部基线 ▼]│  │ # Skill      分数  用例  状态  │ │
│  [全部用途 ▼]│  │ 1 Alpha v1   85    2/2   ✓    │ │
│  [全部时间 ▼]│  │ 2 Beta v1    62    2/2   ✓    │ │
│  ☑ 包含过期  │  └────────────────────────────────┘ │
│              │                                    │
│  ─ 视图 ──  │                                    │
│ [排名][时间] │                                    │
│              │                                    │
│  [× 清除]   │                                    │
│  [↓ 导出CSV]│                                    │
└──────────────┴────────────────────────────────────┘
```

### 12.2 过滤条件

| 控件 | 功能 |
|------|------|
| **搜索框** | 按 Skill 名称模糊搜索 |
| **全部基线** | 只看某条基线的成绩 |
| **全部用途** | 按用途（coding / writing 等）过滤 |
| **全部时间** | 只看近 30 天 / 近 90 天的记录 |
| **包含过期成绩** | 取消勾选后只显示版本仍是最新的成绩 |
| **× 清除** | 重置所有过滤条件 |

### 12.3 排名视图

默认按基线分组显示，每组显示该基线下所有 Skill 的排名：

- **排名序号**：1、2、3…
- **Skill 名称**：当前版本号
- **得分**：最优平均分（100 分制）
- **用例数**：完成用例 / 总用例数
- **新鲜度**：成绩的有效性状态（见下文）

点击任意一行可展开 **6 维度得分详情**。

### 12.4 新鲜度状态说明

| 图标 | 状态 | 含义 |
|------|------|------|
| **✓** | 当前有效 | Skill 和基线版本均未变，成绩完全有效 |
| **⚠** | Skill 已更新 | Skill 在测试后有新版本，成绩对应旧版本 |
| **◆** | 基线已更新 | 测试用例已修改，旧成绩的参照标准已变 |
| **✕** | 全部已更新 | Skill 和基线均已更新，成绩仅作历史参考 |

> **建议**：出现 ⚠ 或 ◆ 时，重新对该 Skill 执行一次测试以获取最新有效成绩。

### 12.5 时间线视图

1. 在左侧点击 **时间线** 按钮切换视图
2. 选择一条基线（必须选择，否则显示提示）
3. 图表以时间为横轴、得分为纵轴，每个 Skill 显示为一条折线
4. 可直观看到每个 Skill 随时间的成绩变化趋势

### 12.6 导出成绩

点击左侧 **↓ 导出 CSV** 按钮，将当前过滤条件下的所有成绩记录导出为 CSV 文件，自动保存到 `workspace/` 目录，并弹出成功通知（含文件路径）。

---

## 13. 数据管理与备份

### 13.1 数据存储位置

所有数据存储在项目根目录的 `workspace/` 文件夹下：

```
workspace/
  skills/          ← 所有 Skill 文件（按 purpose/provider 分层）
  baselines/       ← 所有基线文件
  projects/        ← 所有测试项目
  cli/             ← CLI 配置和会话缓存
  logs/            ← 操作日志（JSONL 格式，每次启动一个文件）
```

### 13.2 手动备份

直接复制整个 `workspace/` 文件夹即可完整备份所有数据。

```
# 示例：将 workspace 备份到桌面
xcopy /E /I workspace "C:\Users\你的用户名\Desktop\skillmanager_backup"
```

### 13.3 恢复数据

将备份的 `workspace/` 文件夹复制回项目根目录，重启应用即可恢复。

### 13.4 查看操作日志

日志文件位于 `workspace/logs/`，以 `YYYY-MM-DD_HH-MM-SS.jsonl` 命名，每行记录一条操作（JSON 格式），可用文本编辑器查看。

---

## 14. 常见问题排查

### ❓ 应用启动后空白或无法显示

**原因**：依赖未正确安装。

**解决**：
```
npm install
npm run dev
```

---

### ❓ 顶部 CLI 状态显示红色（离线）

**原因**：Claude Code CLI 未安装、未在 PATH 中，或未完成授权。

**解决**：
1. 在命令提示符中运行 `claude --version`
   - 若报错 "不是内部或外部命令"：重新安装 `npm install -g @anthropic-ai/claude-code`，然后重启命令提示符
   - 若有版本号但仍为红色：运行 `claude auth login` 完成授权
2. 重启应用

---

### ❓ 点击"Start Test"后提示失败或无响应

**原因**：Claude CLI 不可用、网络问题或 API 配额耗尽。

**解决**：
1. 确认 CLI 状态指示灯为绿色
2. 确认网络可访问 Anthropic API
3. 查看 `workspace/logs/` 下最新日志文件，找到错误信息

---

### ❓ 自动打标签一直显示"进行中"

**原因**：CLI 调用超时（通常在 60 秒内完成）。

**解决**：
1. 等待约 2 分钟
2. 若仍未完成，刷新页面
3. 检查 `workspace/skills/.../<skill_name>/auto_tag_log/` 下的日志文件

---

### ❓ 搜索不到刚导入的 Skill

**原因**：刚导入后列表可能需要刷新。

**解决**：切换到其他页面再切换回来，或滚动刷新列表。

---

### ❓ 需要完全重置数据

**警告**：此操作不可恢复，会删除所有 Skill、Baseline、Project 数据。

```
# 在项目根目录下执行
rmdir /S /Q workspace
```

重启应用后，`workspace/` 会自动重新初始化为空白状态。

---

### ❓ E2E 测试报错 `connect ECONNREFUSED localhost:9222`

此问题仅影响**开发者运行自动化测试**，不影响正常使用。

**解决**：确保运行测试前没有其他 SkillManager 实例在运行，且端口 9222 未被占用。

---

## 附录：完整使用流程示例

以下是一个完整的"从零开始优化提示词"示例流程：

**目标**：有两个写 Python 代码的提示词（Skill A 和 Skill B），想找出哪个更好，并生成一个融合了两者优点的更强版本。

**第一步：导入 Skills**
1. 进入 Skills & Agents 页面
2. 分别导入 Skill A 和 Skill B，Purpose 填 `coding`，Provider 填 `internal`

**第二步：创建测试基线**
1. 进入 Baselines 页面
2. 点击 + Import，手动录入 5 条 Python 编程测试用例
3. 每条用例包含：题目描述（输入）+ 期望的正确代码（预期输出）

**第三步：创建测试项目**
1. 进入 Projects 页面，点击 + Create Project
2. 勾选 Skill A 和 Skill B，勾选刚创建的基线
3. 点击 Create

**第四步：执行测试**
1. 进入项目详情 → Test 标签
2. 点击 Start Test，等待测试完成（约 2-5 分钟）
3. 查看排名，了解哪个 Skill 总分更高

**第五步：分析差异**
1. 进入 Analysis 标签，点击 Run Analysis
2. 查看分析报告，了解 Skill A 在哪些维度领先，Skill B 在哪些维度领先

**第六步：重组优化**
1. 进入 Recompose 标签，点击 Execute Recompose
2. 查看生成的融合版本，根据需要手动微调
3. 保存为新 Skill（如"Python Coding v_optimized"）

**第七步：迭代提升（可选）**
1. 进入 Iteration 标签，选择 Explore 模式
2. 设置 Max Rounds = 5，Stop Threshold = 90
3. 点击 Start Iteration，等待自动迭代完成
4. 查看报告，保存最优版本

**第八步：查看成绩榜**
1. 进入 Rankings 页面
2. 选择基线过滤，对比三个 Skill（A、B、融合版）的得分
3. 切换时间线视图，直观看到分数提升轨迹

---

*文档版本：1.0 · 最后更新：2026-02-28*
